{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/villifCoder559/VGGFaceFeatures_PCA/blob/main/VGGFace.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61gnvBUPhJNr"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -Uqq ipdb\n",
        "import ipdb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yjaRyJjosHEd"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install faiss-cpu --no-cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvE2z802PlYv"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install git+https://github.com/rcmalli/keras-vggface.git\n",
        "!pip install tensorflow keras_applications\n",
        "!pip install layer_utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4_W4a6SCKMr"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install keras==2.12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPyZ0drGU3D5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import keras\n",
        "from keras_vggface.vggface import VGGFace\n",
        "from sklearn.datasets import fetch_lfw_people\n",
        "from skimage.transform import resize\n",
        "import faiss\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import metrics\n",
        "import pandas as pd\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sy0QwabIes5O"
      },
      "source": [
        "<h1>Balancing Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IddmW05E2Zyp"
      },
      "outputs": [],
      "source": [
        "def init_balanced_dataset(lfw_people,balanced_dataset=True):\n",
        "  np.random.seed(42)\n",
        "  data=lfw_people.images\n",
        "  target=lfw_people.target\n",
        "  index_list,nr_elements_per_class=np.unique(target,return_counts=True)\n",
        "  min_elements=min(nr_elements_per_class)\n",
        "  balanced_data=[]\n",
        "  balanced_target=[]\n",
        "  if balanced_dataset:\n",
        "    for s_class in index_list:\n",
        "      list_indices = np.argwhere(target == s_class).reshape(-1)\n",
        "      choices = np.random.choice(list_indices, min_elements, replace = False)\n",
        "      balanced_target.append(np.full(min_elements,s_class))\n",
        "      balanced_data.append(data[choices])\n",
        "    balanced_data=np.array(balanced_data)\n",
        "    balanced_target=np.array(balanced_target)\n",
        "    balanced_data=balanced_data.reshape(balanced_data.shape[0]*balanced_data.shape[1],balanced_data.shape[2],balanced_data.shape[3],balanced_data.shape[4])\n",
        "    balanced_target=balanced_target.reshape(-1)\n",
        "    print('balancedData',balanced_data.shape)\n",
        "    print('balancedTarget',balanced_target.shape)\n",
        "    print()\n",
        "  else:\n",
        "    balanced_data=data\n",
        "    balanced_target=target\n",
        "    print('balancedData',balanced_data.shape)\n",
        "    print('balancedTarget',balanced_target.shape)\n",
        "  return balanced_data,balanced_target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glcX5CjKiDI2"
      },
      "source": [
        "<h1>Image normalization and standardization and Extract features</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmuLa6h2k5zX"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "def compute_features(balanced_data,balanced_target,use_VGGface=True,random_state=2,std_dataset=True):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(balanced_data, balanced_target, test_size=0.2, random_state=random_state)\n",
        "\n",
        "  image_samples_train=np.copy(X_train)\n",
        "  if std_dataset:\n",
        "    image_samples_train=np.array([cv2.resize(img,(224,224)) for img in image_samples_train])\n",
        "    image_samples_train[:,:,:,0]=(image_samples_train[:,:,:,0]-np.mean(X_train[:,:,:,0]))/(np.std(X_train[:,:,:,0]))\n",
        "    image_samples_train[:,:,:,1]=(image_samples_train[:,:,:,1]-np.mean(X_train[:,:,:,1]))/(np.std(X_train[:,:,:,1]))\n",
        "    image_samples_train[:,:,:,2]=(image_samples_train[:,:,:,2]-np.mean(X_train[:,:,:,2]))/(np.std(X_train[:,:,:,2]))\n",
        "\n",
        "  image_samples_test=np.copy(X_test)\n",
        "  if(std_dataset):\n",
        "    image_samples_test=np.array([cv2.resize(img,(224,224)) for img in image_samples_test])\n",
        "    image_samples_test[:,:,:,0]=(image_samples_test[:,:,:,0]-np.mean(X_train[:,:,:,0]))/(np.std(X_train[:,:,:,0]))\n",
        "    image_samples_test[:,:,:,1]=(image_samples_test[:,:,:,1]-np.mean(X_train[:,:,:,1]))/(np.std(X_train[:,:,:,1]))\n",
        "    image_samples_test[:,:,:,2]=(image_samples_test[:,:,:,2]-np.mean(X_train[:,:,:,2]))/(np.std(X_train[:,:,:,2]))\n",
        "  if use_VGGface:\n",
        "    if tf.config.list_physical_devices('GPU'):\n",
        "      vggface = VGGFace(model='vgg16',include_top=False,input_shape=(image_samples_train.shape[1],image_samples_train.shape[2],image_samples_train.shape[3]))\n",
        "      result_train=vggface.predict(image_samples_train)\n",
        "      result_test=vggface.predict(image_samples_test)\n",
        "      print('GPU available')\n",
        "    else:\n",
        "      drive.mount('/content/drive')\n",
        "      drive_path = '/content/drive/MyDrive/'\n",
        "      result_train=np.load(os.path.join(drive_path, 'VGGFace_features_faces_norm_std_train.npy'),allow_pickle=True)\n",
        "      result_test=np.load(os.path.join(drive_path, 'VGGFace_features_faces_norm_std_test.npy'),allow_pickle=True)\n",
        "      print('GPU not available. Getting features from local BALANCED dataset (train_test_split with random_state=2)...')\n",
        "  else:\n",
        "    result_train=image_samples_train\n",
        "    result_test=image_samples_test\n",
        "  return result_train,result_test,y_test,y_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LscqwWAMkmVU"
      },
      "source": [
        "<h1>Save/load features from drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFvqiw-31-z5"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# import os\n",
        "# drive.mount('/content/drive')\n",
        "# drive_path = '/content/drive/MyDrive/'\n",
        "# np.save(os.path.join(drive_path, 'VGGFace_features_faces_norm_std_train.npy'), original_result_train)\n",
        "# np.save(os.path.join(drive_path, 'VGGFace_features_faces_norm_std_test.npy'), original_result_test)\n",
        "# np.save(os.path.join(drive_path, 'VGGFace_features_faces_train.npy'), original_result_train)\n",
        "# np.save(os.path.join(drive_path, 'VGGFace_features_faces_test.npy'), original_result_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ylhbR17i2uDM"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# import os\n",
        "# drive.mount('/content/drive')\n",
        "# drive_path = '/content/drive/MyDrive/'\n",
        "# result_train_vgg=np.load(os.path.join(drive_path, 'VGGFace_features_faces_norm_std_train.npy'),allow_pickle=True)\n",
        "# result_test_vgg=np.load(os.path.join(drive_path, 'VGGFace_features_faces_norm_std_test.npy'),allow_pickle=True)\n",
        "# result_train_vgg=np.load(os.path.join(drive_path, 'VGGFace_features_faces_train.npy'),allow_pickle=True)\n",
        "# result_test_vgg=np.load(os.path.join(drive_path, 'VGGFace_features_faces_test.npy'),allow_pickle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1RKwLuhXeDy"
      },
      "outputs": [],
      "source": [
        "#create reduction (if n_components is negative it means to remove the first n_components instead of lasts)\n",
        "def create_test_reduction(n_components,result_train,type_index='FlatL2',fixed_components=30):\n",
        "  if type_index not in ['FlatL2', 'FlatIP']:\n",
        "        raise ValueError(\"Type must be 'FlatL2' or 'FlatIP'\")\n",
        "  features_reducted=result_train.reshape(result_train.shape[0],-1)\n",
        "  if n_components > 0:\n",
        "    tmp_PCA=PCA(n_components=n_components)\n",
        "    features_reducted=np.ascontiguousarray(tmp_PCA.fit_transform(features_reducted))\n",
        "  else:\n",
        "    tmp_PCA=PCA(n_components=fixed_components).fit(features_reducted)\n",
        "    features_reducted=custom_reduction(tmp_PCA,result_train,n_components)\n",
        "  faiss.normalize_L2(features_reducted)\n",
        "  if type_index=='FlatIP':\n",
        "    return tmp_PCA,features_reducted,faiss.IndexFlatIP(features_reducted.shape[1])\n",
        "  else:\n",
        "    return tmp_PCA,features_reducted,faiss.IndexFlatL2(features_reducted.shape[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7c8Y3ky9fZSf"
      },
      "outputs": [],
      "source": [
        "def custom_reduction(pca,result_train,start_index):\n",
        "  transformed_results = []\n",
        "  for element in result_train:\n",
        "    test = element.reshape(1, -1)\n",
        "    # Apply PCA transformation\n",
        "    transformed_result = np.dot(test - pca.mean_, pca.components_[(-start_index):].T)\n",
        "    transformed_results.append(transformed_result)\n",
        "  return np.array(transformed_results).reshape(result_train.shape[0],pca.components_.shape[0]+start_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hdv6dHVIfMTn"
      },
      "source": [
        "<h1>Create Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wcs4ZeZcJX4G"
      },
      "outputs": [],
      "source": [
        "def create_test(list_n_components,fixed_components,result_train_vgg):\n",
        "  index_type='FlatIP' # FlatIP or FlatL2\n",
        "  print('Fixed_components=',fixed_components)\n",
        "  pca_objs=[]\n",
        "  features_objs=[]\n",
        "  index_faiss_objs=[]\n",
        "  for n_components in list_n_components:\n",
        "    if n_components>0:\n",
        "      pca,features,index=create_test_reduction(n_components,result_train_vgg,index_type)\n",
        "      pca_objs.append(pca)\n",
        "      features_objs.append(features)\n",
        "      index_faiss_objs.append(index)\n",
        "    else:\n",
        "      if(n_components<0):\n",
        "        pca,features,index=create_test_reduction(n_components,result_train_vgg,index_type,fixed_components)\n",
        "        pca_objs.append(pca)\n",
        "        features_objs.append(features)\n",
        "        index_faiss_objs.append(index)\n",
        "      else: # dummy PCA\n",
        "        pca_objs.append(PCA())\n",
        "        res_train_norm=np.copy(result_train_vgg.reshape(result_train_vgg.shape[0],-1))\n",
        "        faiss.normalize_L2(res_train_norm)\n",
        "        features_objs.append(res_train_norm)\n",
        "        if index_type=='FlatIP':\n",
        "          index_faiss_objs.append(faiss.IndexFlatIP(res_train_norm.shape[1]))\n",
        "        else:\n",
        "          index_faiss_objs.append(faiss.IndexFlatL2(res_train_norm.shape[1]))\n",
        "  #add vectors to index\n",
        "  count=0\n",
        "  for index in index_faiss_objs:\n",
        "    # print(features_objs[count].shape)\n",
        "    index.add(features_objs[count])\n",
        "    count=count+1\n",
        "  return pca_objs,features_objs,index_faiss_objs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jzd9K7Oml09B"
      },
      "outputs": [],
      "source": [
        "def compute_precision_recall(ground_truth,target,index,top_k,reduced_vector,train_elements_nr):\n",
        "  TP=-1\n",
        "  reduced_vector=np.ascontiguousarray(reduced_vector.reshape(1, -1))\n",
        "  # print(reduced_vector.shape)\n",
        "  _,retrived_elements=index.search(reduced_vector, train_elements_nr)\n",
        "  retrived_targets_id=ground_truth[retrived_elements]==target\n",
        "  # print(retrived_elements)\n",
        "  indices = np.where(retrived_targets_id.reshape(-1))[0]\n",
        "  if(top_k>len(indices)):\n",
        "    # print('Face nr.',target,'max elements:',len(indices)-1,'(NOT',str(top_k)+')')\n",
        "    top_k=len(indices)\n",
        "  stop=indices[top_k-1]+1\n",
        "  TP=top_k\n",
        "  total_samples=np.sum(ground_truth==target)\n",
        "  FP=stop-TP\n",
        "  FN=total_samples-TP\n",
        "  precision=0\n",
        "  if TP+FP!=0:\n",
        "    precision=TP/(TP+FP)\n",
        "  recall=TP/((TP+FN))\n",
        "  return precision,recall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdGoaALBohJf"
      },
      "outputs": [],
      "source": [
        "def predict(ground_truth_train,ground_truth_test,index_faiss_objs,test_vectors,list_n_components,train_elements_nr=9):\n",
        "  i=0\n",
        "  j=0\n",
        "  test_vectors=test_vectors.reshape(test_vectors.shape[0],-1)\n",
        "  nr_labels=np.unique(y_train).shape[0]\n",
        "  result=[]\n",
        "  list_results=[]\n",
        "\n",
        "  for index in index_faiss_objs:\n",
        "      # start=time.time()\n",
        "      for vector in test_vectors:\n",
        "        # print(list_n_components[i])\n",
        "        if list_n_components[i] > 0:\n",
        "          reduced_vector=(vector.reshape(1,-1)-pca_objs[i].mean_)@pca_objs[i].components_.T\n",
        "        else:\n",
        "          if list_n_components[i] < 0:\n",
        "            reduced_vector=(vector.reshape(1,-1)-pca_objs[i].mean_)@pca_objs[i].components_[-list_n_components[i]:].T\n",
        "          else:\n",
        "            reduced_vector=vector\n",
        "            reduced_vector=np.reshape(reduced_vector,(1,reduced_vector.shape[0]))\n",
        "        reduced_vector=reduced_vector/np.linalg.norm(reduced_vector)\n",
        "        # ipdb.set_trace()\n",
        "        distances,retrived_elements=index.search(reduced_vector, train_elements_nr)\n",
        "        sum_per_class=[np.sum(ground_truth_train[retrived_elements]==i) for i in range(nr_labels)]\n",
        "        max_indices = np.where(sum_per_class == np.max(sum_per_class))[0]\n",
        "        if len(max_indices) > 1:  # If multiple classes have the same sum\n",
        "          index_vct=[(ground_truth_train[retrived_elements]==i) for i in (max_indices)]\n",
        "          dist=[np.sum(distances[vector]) for vector in index_vct]\n",
        "          if(index.__class__.__name__ == 'IndexFlatIP'):\n",
        "            max_index=np.argmax(dist)\n",
        "            best=max_indices[max_index]\n",
        "          else:\n",
        "            min_index=np.argmin(dist)\n",
        "            best=max_indices[min_index]\n",
        "          # ipdb.set_trace()\n",
        "        else:\n",
        "          best = max_indices[0]\n",
        "        result.append(best)\n",
        "        j+=1\n",
        "      j=0\n",
        "      i=i+1\n",
        "      result=np.array(result)\n",
        "      accuracy=np.sum(result==ground_truth_test)/result.shape[0]\n",
        "      list_results.append(accuracy)\n",
        "      result=[]\n",
        "  return list_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1QvvIAafYHQ"
      },
      "source": [
        "<h1>Compute Precision-recall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WpBTT2YzqBTP"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "def compute_all_precision_recall(list_top_elements,list_n_components,index_faiss_objs,pca_objs,y_train,y_test,result_test_vgg,result_train):\n",
        "  # list_top_elements=[5,10,20,100]\n",
        "  i=0\n",
        "  j=0\n",
        "  k=0\n",
        "  precision_results=[]\n",
        "  recall_results=[]\n",
        "  name_results=[]\n",
        "  list_precision_results=[]\n",
        "  list_recall_results=[]\n",
        "  # reduced_vector_list=[]\n",
        "  test_vectors=result_test_vgg.reshape(result_test_vgg.shape[0],-1)\n",
        "  for top_k in list_top_elements:\n",
        "    for index in index_faiss_objs:\n",
        "      # start=time.time()\n",
        "      for vector in test_vectors:\n",
        "        if list_n_components[i] > 0:\n",
        "          reduced_vector=(vector.reshape(1,-1)-pca_objs[i].mean_)@pca_objs[i].components_.T\n",
        "        else:\n",
        "          if list_n_components[i] < 0:\n",
        "            reduced_vector=(vector.reshape(1,-1)-pca_objs[i].mean_)@pca_objs[i].components_[-list_n_components[i]:].T\n",
        "          else:\n",
        "            reduced_vector=np.array(vector)\n",
        "        reduced_vector=reduced_vector/np.linalg.norm(reduced_vector)\n",
        "        # reduced_vector_list.append(reduced_vector)\n",
        "        # predict(y_train,index,reduced_vector,9)\n",
        "        precision,recall=compute_precision_recall(y_train,y_test[j],index,top_k,reduced_vector,result_train.shape[0])\n",
        "        name_results.append(y_test[j])\n",
        "        precision_results.append(precision)\n",
        "        recall_results.append(recall)\n",
        "        j=j+1\n",
        "      j=0\n",
        "      i=i+1\n",
        "    i=0\n",
        "      # tmp=time.time()-start\n",
        "  recall_results=np.array(recall_results).reshape(len(list_top_elements),len(index_faiss_objs),test_vectors.shape[0])\n",
        "  precision_results=np.array(precision_results).reshape(len(list_top_elements),len(index_faiss_objs),test_vectors.shape[0])\n",
        "  name_results=np.array(name_results).reshape(len(list_top_elements),len(index_faiss_objs),test_vectors.shape[0])\n",
        "  return recall_results,precision_results,name_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jh9hY2qno4n4"
      },
      "outputs": [],
      "source": [
        "def create_DataFrame(id_labels,selected_name_result,selected_precision_result,selected_recall_result,normalization_factors,index_faiss_n_components,list_n_components,index_selected_top_k_retrival):\n",
        "  str_top_k=str(list_top_elements[index_selected_top_k_retrival])\n",
        "  text=''\n",
        "  index_type=''\n",
        "  precision_vector=np.bincount(selected_name_result,weights=selected_precision_result)/normalization_factors\n",
        "  recall_vector=np.bincount(selected_name_result,weights=selected_recall_result)/normalization_factors\n",
        "  # F1_vector=2*(precision_vector*recall_vector)/(precision_vector+recall_vector)\n",
        "  if index_faiss_objs[index_faiss_n_components].__class__.__name__ == 'IndexFlatIP':\n",
        "    index_type='(IP)'\n",
        "  else:\n",
        "    index_type='(L2)'\n",
        "  if list_n_components[index_faiss_n_components]<0:\n",
        "    text='/'+str(fixed_components)\n",
        "  df = pd.DataFrame({\n",
        "    \"Faces\":id_labels,\n",
        "    # (\"F\"+index_type+\"@\"+str_top_k+'_'+str(list_n_components[index_faiss_n_components])+text):F1_vector        # F1\n",
        "    (\"P\"+index_type+\"@\"+str_top_k+'_'+str(list_n_components[index_faiss_n_components])+text):precision_vector,  # Precision\n",
        "    # (\"R@\"+str_top_k+'_'+str(list_n_components[index_faiss_n_components])):recall_vector,                      # Recall\n",
        "  })\n",
        "  # Calculate the averages of each column\n",
        "  avg_row = df.mean(axis=0)\n",
        "  avg_row[\"Faces\"] = \"Average\"\n",
        "  # Append the average row to the DataFrame\n",
        "  # df = pd.concat([df,avg_row],ignore_index=True)\n",
        "  df = df.append(avg_row, ignore_index=True)\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wccn9Ttfn_LI"
      },
      "outputs": [],
      "source": [
        "def create_list_dataframe(precision_results,name_results,recall_results,list_n_components,y_test,list_top_elements):\n",
        "  df_list=[]\n",
        "  # avg_precision=[]\n",
        "  for i in range(precision_results.shape[0]):\n",
        "    for j in range(precision_results.shape[1]):\n",
        "      selected_name_result = name_results[i][j]\n",
        "      selected_recall_result = recall_results[i][j]\n",
        "      selected_precision_result = precision_results[i][j]\n",
        "      # avg_precision.append(np.mean(selected_precision_result))\n",
        "      id_labels,normalization_factors = np.unique(y_test,return_counts=True)\n",
        "      df=create_DataFrame(id_labels,selected_name_result,selected_precision_result,selected_recall_result,normalization_factors,j,list_n_components,i)\n",
        "      df_list.append(df)\n",
        "  # avg_precision=np.array(avg_precision).reshape(len(list_top_elements),len(list_n_components))\n",
        "  return df_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Prq_Z7xFASMq"
      },
      "source": [
        "F1 Score all retrived<br>\n",
        "P(IP)@1000_0    -> F-Score(distance_InnerProduct)@all_elements_noPCAReduction<br>\n",
        "P(IP)@1000_200  ->F-Score(distance_InnerProduct)@all_elements_PCA_200_Components<br>\n",
        "P(IP)@1000_-2/25-> F-Score(Distance_InnerProduct)@W/h first 2 componets out of 25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTYR-O2eVwTO"
      },
      "outputs": [],
      "source": [
        "def merge_dataframe(list_top_elements,df_list,drop_faces_col=True):\n",
        "  merged_df_list=[]\n",
        "  count=0\n",
        "  for pick_top_k in list_top_elements:\n",
        "    merged_df=df_list[count]\n",
        "    for i in range(count+1,count+int(len(df_list)/len(list_top_elements))):\n",
        "      merged_df=pd.merge(merged_df,df_list[i])\n",
        "    if drop_faces_col:\n",
        "      merged_df_list.append(merged_df.drop(columns=['Faces']).style.highlight_min(color = 'red', axis = 1).highlight_max(color = 'green', axis = 1))\n",
        "    else:\n",
        "      merged_df_list.append(merged_df)\n",
        "    count+=int(len(df_list)/len(list_top_elements))\n",
        "  return merged_df_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jww-7Tt_oPKq"
      },
      "outputs": [],
      "source": [
        "def compute_fisher(pca_objs,list_n_components,result_train,result_test,y_train):\n",
        "  accuracy_results=[]\n",
        "  count=0\n",
        "  # for n_components in list_n_components:\n",
        "  for pca in pca_objs:\n",
        "    print('PCA components',list_n_components[count])\n",
        "    eigenface_recognizer = cv2.face.EigenFaceRecognizer_create()\n",
        "    if list_n_components[count]>0:\n",
        "      train=pca.fit_transform(result_train.reshape(result_train.shape[0],-1))\n",
        "    else:\n",
        "      train=result_train.reshape(result_train.shape[0],-1)\n",
        "    eigenface_recognizer.train(train, np.array(y_train))\n",
        "    correct = 0\n",
        "    if list_n_components[count]>0:\n",
        "      test=pca.transform(result_test.reshape(result_test.shape[0],-1))\n",
        "    else:\n",
        "      test=result_test.reshape(result_test.shape[0],-1)\n",
        "    total = len(test)\n",
        "    for i in range(total):\n",
        "        label, confidence = eigenface_recognizer.predict(test[i])\n",
        "        if label == y_test[i]:\n",
        "            correct += 1\n",
        "    accuracy = correct / total\n",
        "    accuracy_results.append(accuracy)\n",
        "    # print(f\"Accuracy: {accuracy}%\")\n",
        "    # print()\n",
        "  return np.array(accuracy_results).reshape(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvG4aRieMRTU"
      },
      "outputs": [],
      "source": [
        "lfw_people = fetch_lfw_people(min_faces_per_person=30, resize=2.4,color=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(lfw_people.target_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2arm7SQrTGvf",
        "outputId": "540a53e1-2bae-4c64-9ef4-fd7da112f9ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "34"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLjbpx_EG3tG"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "list_n_components = [0,20,50,100,396] # 0 -> No PCA reduction\n",
        "list_top_elements = [5,10,20,100]\n",
        "fixed_components_list = [20]\n",
        "random_state_list= range(1,10)\n",
        "predictions_vgg=[]\n",
        "predictions_fisher=[]\n",
        "list_precision_results=[]\n",
        "make_prediction=True\n",
        "balanced_data,balanced_target=init_balanced_dataset(lfw_people,balanced_dataset=True)\n",
        "for random_state in random_state_list:\n",
        "  start=time.time()\n",
        "  for fixed_components in fixed_components_list:\n",
        "    result_train,result_test,y_test,y_train=compute_features(balanced_data,balanced_target,use_VGGface=True,random_state=random_state,std_dataset=True)\n",
        "    pca_objs, features_objs, index_faiss_objs = create_test(list_n_components,fixed_components,result_train)\n",
        "    if make_prediction:\n",
        "      predictions_vgg.append(predict(y_train,y_test,index_faiss_objs,result_test,list_n_components))\n",
        "      # predictions_fisher.append(compute_fisher(pca_objs,list_n_components,result_train,result_test,y_train))\n",
        "    else:\n",
        "      recall_results, precision_results, name_results = compute_all_precision_recall(list_top_elements,list_n_components,index_faiss_objs,pca_objs,y_train,y_test,result_test,result_train)\n",
        "      list_precision_results.append(precision_results)\n",
        "      df_list=create_list_dataframe(precision_results,name_results,recall_results,list_n_components,y_test,list_top_elements)\n",
        "      merged_df_list=merge_dataframe(list_top_elements,df_list,drop_faces_col=True)\n",
        "  print('random_state '+str(random_state)+' time -> '+str(start-time.time()))\n",
        "if make_prediction:\n",
        "  predictions_vgg=np.array(predictions_vgg)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_vgg #min_faces=30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhuEMpfJOM20",
        "outputId": "c9c30e53-8db5-4e4b-a629-4be2a87e97d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.96568627, 0.90196078, 0.94117647, 0.96078431, 0.96078431],\n",
              "       [0.97058824, 0.91176471, 0.92647059, 0.93627451, 0.95098039],\n",
              "       [0.99019608, 0.93137255, 0.96078431, 0.96078431, 0.97058824],\n",
              "       [0.98039216, 0.93137255, 0.95098039, 0.96078431, 0.96568627],\n",
              "       [0.9754902 , 0.95588235, 0.96568627, 0.96078431, 0.96078431],\n",
              "       [0.98529412, 0.96568627, 0.96078431, 0.96568627, 0.98039216],\n",
              "       [0.98039216, 0.92647059, 0.9754902 , 0.98039216, 0.98529412],\n",
              "       [0.99019608, 0.93627451, 0.96568627, 0.97058824, 0.98039216],\n",
              "       [0.98039216, 0.95098039, 0.95588235, 0.96078431, 0.9754902 ]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_vgg #balanced"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_YuJG2142zT",
        "outputId": "116e37a4-29da-45a6-b67d-dcf3c28e41f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.96568627, 0.90196078, 0.94117647, 0.96078431, 0.96078431],\n",
              "       [0.97058824, 0.91176471, 0.92647059, 0.93627451, 0.95098039],\n",
              "       [0.99019608, 0.93137255, 0.96078431, 0.96078431, 0.97058824],\n",
              "       [0.98039216, 0.93137255, 0.95098039, 0.96078431, 0.96568627],\n",
              "       [0.9754902 , 0.95588235, 0.96568627, 0.96078431, 0.96078431],\n",
              "       [0.98529412, 0.96568627, 0.96078431, 0.96568627, 0.98039216],\n",
              "       [0.98039216, 0.92647059, 0.9754902 , 0.98039216, 0.98529412],\n",
              "       [0.99019608, 0.93627451, 0.96568627, 0.97058824, 0.98039216],\n",
              "       [0.98039216, 0.95098039, 0.95588235, 0.96078431, 0.9754902 ]])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_vgg #unbalanced"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fry9M_zu9exY",
        "outputId": "89f92371-c705-4ffe-df1f-fb926125b03e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.96568627, 0.90196078, 0.94117647, 0.96078431, 0.96078431],\n",
              "       [0.97058824, 0.91176471, 0.92647059, 0.93627451, 0.95098039],\n",
              "       [0.99019608, 0.93137255, 0.96078431, 0.96078431, 0.97058824],\n",
              "       [0.98039216, 0.93137255, 0.95098039, 0.96078431, 0.96568627],\n",
              "       [0.9754902 , 0.95588235, 0.96568627, 0.96078431, 0.96078431],\n",
              "       [0.98529412, 0.96568627, 0.96078431, 0.96568627, 0.98039216],\n",
              "       [0.98039216, 0.92647059, 0.9754902 , 0.98039216, 0.98529412],\n",
              "       [0.99019608, 0.93627451, 0.96568627, 0.97058824, 0.98039216],\n",
              "       [0.98039216, 0.95098039, 0.95588235, 0.96078431, 0.9754902 ]])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(predictions_vgg,axis=0)"
      ],
      "metadata": {
        "id": "JTtS2RclrnyY",
        "outputId": "2b225128-712b-402b-93fa-4aeb6581902a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.97984749, 0.93464052, 0.95588235, 0.96187364, 0.97004357])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "No801t81O7Ck"
      },
      "outputs": [],
      "source": [
        "merged_df_list[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hn034EVIO95V"
      },
      "outputs": [],
      "source": [
        "merged_df_list[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WAhvdLPPAC9"
      },
      "outputs": [],
      "source": [
        "merged_df_list[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNabDmJMPBnh"
      },
      "outputs": [],
      "source": [
        "merged_df_list[3]"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}